---
 tags:
  - LinearAlgebra
---



SVD 是线性代数“基本定理”的最终形式。

### 完美矩阵 vs 一般矩阵

- 完美情况（对称矩阵 $A=A^T$）：

  我们可以找到一组正交的特征向量，将矩阵分解为 $A = Q \Lambda Q^T$。

  - 基 (Basis) 是正交的 ($Q$)。
  - 缩放因子是特征值 ($\lambda$)。

- **一般情况（任意矩阵 $A$）**：

  - 通常的特征分解 $A = S \Lambda S^{-1}$ 使用的是特征向量矩阵 $S$。
  - **缺陷**：对于非对称矩阵，$S$ 的列向量**不是正交的**。这意味着我们在扭曲的坐标系下看问题，不够完美。

### SVD 的核心改进：两组基

为了对任意矩阵（甚至是长方形矩阵）实现“对角化”，我们必须做出一个妥协，但也是一种突破：

我们在输入空间和输出空间使用两组不同的正交基。

- **输入空间 ($\mathbb{R}^n$)**：使用一组正交基 $v_1, v_2, \dots, v_n$。
- **输出空间 ($\mathbb{R}^m$)**：使用另一组正交基 $u_1, u_2, \dots, u_m$。

------

## 几何图景：从圆到椭圆 (Circle to Ellipse)

### 线性变换的作用

想象在输入空间 $\mathbb{R}^n$ 中有一个单位圆（Unit Circle）（或者超球面）。

当矩阵 $A$ 作用于这个圆上的每一个向量 $x$ 时，得到的图像 $Ax$ 会是什么形状？

- **结论**：$A$ 将 $\mathbb{R}^n$ 中的一个**圆**变成了 $\mathbb{R}^m$ 中的一个**椭圆 (Hyper-ellipse)**。

### 寻找特殊的轴

在这个变换过程中，圆上大部分向量在这个过程中都旋转了，方向改变了。但是，对于椭圆来说，有几个特定的方向特别重要——即**椭圆的长轴和短轴**。

SVD 的任务就是找到这些轴：

1. **$v_i$ (Right Singular Vectors)**：是输入空间（圆）上的特定单位向量。它们经过 $A$ 变换后，正好落在输出椭圆的主轴方向上。
2. **$u_i$ (Left Singular Vectors)**：是输出空间（椭圆）主轴方向上的单位向量。
3. **$\sigma_i$ (Singular Values)**：是椭圆半轴的长度（即拉伸的倍率）。

![Gemini_Generated_Image_fethckfethckfeth](attachments/Gemini_Generated_Image_fethckfethckfeth.png)



### 数学关系式

根据上述几何描述，我们可以写出：



$$A v_1 = \sigma_1 u_1$$

$$A v_2 = \sigma_2 u_2$$

$$\vdots$$

$$A v_r = \sigma_r u_r$$

其中 $\sigma_1$ 是椭圆最长半轴的长度，$\sigma_2$ 是次长半轴，以此类推。

------

## 代数推导：从向量到矩阵

将上述的向量方程组装成矩阵形式。

### 矩阵形式化

将所有的 $v$ 排列成矩阵 $V$，所有的 $u$ 排列成矩阵 $U$，所有的 $\sigma$ 放入对角矩阵 $\Sigma$。

方程 $Av_i = \sigma_i u_i$ 变为：

$$A \begin{bmatrix} | & & | \\ v_1 & \dots & v_n \\ | & & | \end{bmatrix} = \begin{bmatrix} | & & | \\ u_1 & \dots & u_m \\ | & & | \end{bmatrix} \begin{bmatrix} \sigma_1 & & \\ & \ddots & \\ & & \sigma_n \end{bmatrix}$$

即：



$$AV = U\Sigma$$

### 得到 SVD 公式

由于 $V$ 是我们在输入空间选取的标准正交基（Orthonormal Basis），所以 $V$ 是正交矩阵，满足 $V^{-1} = V^T$。

在等式两边同时右乘 $V^T$：

$$A = U \Sigma V^T$$

这就是 SVD 的代数定义。

### 零空间的补全

Strang 教授特别提到，如果矩阵不是方阵或不满秩：

- 前 $r$ 个 $v$ 和 $u$ 描述了**行空间**到**列空间**的一一映射（椭圆的非零轴）。
- 剩下的 $v_{r+1} \dots v_n$ 属于**零空间** $N(A)$，它们被 $A$ 映射为 0（对应 $\sigma=0$）。
- 剩下的 $u_{r+1} \dots u_m$ 属于**左零空间** $N(A^T)$，用来补全 $\mathbb{R}^m$ 的基。

------

## 为什么要处理 $A^T A$？

（这部分连接了这一讲的前半段和后半段）

既然我们需要 $A = U \Sigma V^T$，我们要如何解出 $U$ 和 $V$？

Strang 教授并没有凭空给出算法，而是通过对称化来寻找线索：

如果 $A$ 是个普通的矩阵，那么 $A^T A$ 一定是一个对称矩阵，而且是半正定矩阵。



$$A^T A = (V \Sigma^T U^T) (U \Sigma V^T)$$



由于 $U$ 是正交的，$U^T U = I$，中间项抵消：



$$A^T A = V (\Sigma^T \Sigma) V^T$$

$$A^T A = V (\Sigma^2) V^{-1}$$

这一步极其精彩，它告诉我们：

1. 我们不需要去解复杂的非线性方程组。
2. 我们只需要对 $A^T A$ 做经典的**特征值分解**。
3. $A^T A$ 的特征向量就是我们苦苦寻找的 $V$。
4. $A^T A$ 的特征值就是 $\sigma^2$。



## SVD 的核心目标

以前学过的分解（如 $A=LDU$ 或 $A=S\Lambda S^{-1}$）都有局限性（例如要求方阵、要求有足够的特征向量等）。

SVD 的目标是：对于任意一个 $m \times n$ 的矩阵 $A$，将其分解为三个特殊矩阵的乘积。

$$A = U \Sigma V^T$$

### 1.1 矩阵成分解析

- **$A$**: 任意实数矩阵 ($m \times n$)。
- **$U$**: 正交矩阵 ($m \times m$)。其列向量称为**左奇异向量** (Left Singular Vectors)。
- **$\Sigma$**: 对角矩阵 ($m \times n$)。对角线元素 $\sigma_i$ 称为**奇异值** (Singular Values)，且 $\sigma_1 \geq \sigma_2 \geq \dots \geq 0$。
- **$V^T$**: 正交矩阵的转置 ($n \times n$)。$V$ 的列向量称为**右奇异向量** (Right Singular Vectors)。

> **注意：** 这里的“对角矩阵”$\Sigma$ 如果是非方阵，意味着只有主对角线（$(1,1), (2,2)...$）上有非零元素，其余全为 0。

------

## 2. 几何解释：SVD 的直观意义

SVD 的本质是寻找两组正交基。

在 $\mathbb{R}^n$ (行空间) 中找到一组正交基 $v_1, \dots, v_n$，经过矩阵 $A$ 变换后，它们变成了 $\mathbb{R}^m$ (列空间) 中的另一组正交基 $u_1, \dots, u_m$，并伴随着长度的伸缩（由 $\sigma$ 决定）。

数学表达为：



$$Av_i = \sigma_i u_i$$

写成矩阵形式：



$$AV = U \Sigma$$

$$\Downarrow$$

$$A = U \Sigma V^{-1} = U \Sigma V^T \quad (\text{因为 } V \text{ 是正交矩阵，} V^{-1} = V^T)$$

**变换过程分解：**

1. **$V^T$ (Rotation):** 在 $\mathbb{R}^n$ 中旋转/反射坐标轴。
2. **$\Sigma$ (Stretching):** 沿坐标轴进行伸缩，拉伸因子为奇异值 $\sigma$。
3. **$U$ (Rotation):** 在 $\mathbb{R}^m$ 中再次旋转/反射得到最终结果。

------

## 3. 推导与计算：如何找到 $U, \Sigma, V$

利用对称矩阵 $A^T A$ 和 $A A^T$ 的性质。

### 3.1 寻找 $V$ 和 $\Sigma$

考察 $A^T A$（这是一个对称正定或半正定矩阵）：



$$A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T$$

因为 $U$ 是正交矩阵，所以 $U^T U = I$。上式简化为：



$$A^T A = V (\Sigma^T \Sigma) V^T$$

这正是 $A^T A$ 的**特征值分解**形式 ($Q \Lambda Q^T$)！

- **$V$ 的列向量**：是 $A^T A$ 的**特征向量**。
- **$\Sigma^2$ 的对角元**：是 $A^T A$ 的**特征值** ($\lambda$)。
- **奇异值 $\sigma_i$**：是 $A^T A$ 特征值的平方根，即 $\sigma_i = \sqrt{\lambda_i}$。

### 3.2 寻找 $U$

同理，我们考察 $A A^T$：



$$A A^T = (U \Sigma V^T)(V \Sigma^T U^T) = U (\Sigma \Sigma^T) U^T$$

因为 $V$ 是正交矩阵，$V^T V = I$。

- **$U$ 的列向量**：是 $A A^T$ 的**特征向量**。

------

## 4. 计算步骤总结 (Algorithm)

假设我们要分解矩阵 $A$：

1. **计算 $A^T A$**。
2. **求特征值和特征向量**：
   - 求出 $A^T A$ 的特征值 $\lambda_1, \dots, \lambda_n$。
   - 计算奇异值 $\sigma_i = \sqrt{\lambda_i}$，并将它们按从大到小填入 $\Sigma$。
   - 求出对应的单位特征向量 $v_1, \dots, v_n$，构成矩阵 $V$。
3. **求 $U$**：
   - **方法一（推荐）**：利用公式 $u_i = \frac{Av_i}{\sigma_i}$ 直接求出前 $r$ 个 $u$（对应非零奇异值）。这样可以确保 $u_i$ 和 $v_i$ 的符号是匹配的。
   - **方法二**：计算 $A A^T$ 的特征向量。**警告**：这种方法无法确定特征向量的符号正负，可能导致最终 $U \Sigma V^T$ 乘积符号错误。
4. **补全零空间**：如果 $A$ 不可逆或非方阵，利用 Gram-Schmidt 正交化补全 $U$ 和 $V$ 中剩余的正交向量（对应零空间的基）。

------

## 5. 课堂经典示例

设 $A = \begin{bmatrix} 4 & 4 \\ -3 & 3 \end{bmatrix}$

1. 计算 $A^T A$:



$$A^T A = \begin{bmatrix} 4 & -3 \\ 4 & 3 \end{bmatrix} \begin{bmatrix} 4 & 4 \\ -3 & 3 \end{bmatrix} = \begin{bmatrix} 25 & 7 \\ 7 & 25 \end{bmatrix}$$

2. 求特征值和奇异值:

特征方程 $\det(A^T A - \lambda I) = \lambda^2 - 50\lambda + 576 = 0$

解得 $\lambda_1 = 32, \lambda_2 = 18$。

奇异值 $\sigma_1 = \sqrt{32} = 4\sqrt{2}, \quad \sigma_2 = \sqrt{18} = 3\sqrt{2}$。

所以 $\Sigma = \begin{bmatrix} 4\sqrt{2} & 0 \\ 0 & 3\sqrt{2} \end{bmatrix}$。

3. 求 $V$ (特征向量):

对应 $\lambda_1=32$，$v_1 = \begin{bmatrix} 1 \\ 1 \end{bmatrix}$ (标准化后 $\frac{1}{\sqrt{2}}\begin{bmatrix} 1 \\ 1 \end{bmatrix}$)。

对应 $\lambda_2=18$，$v_2 = \begin{bmatrix} -1 \\ 1 \end{bmatrix}$ (标准化后 $\frac{1}{\sqrt{2}}\begin{bmatrix} -1 \\ 1 \end{bmatrix}$)。

4. 求 $U$:

利用 $u_i = \frac{Av_i}{\sigma_i}$：



$$Av_1 = \begin{bmatrix} 4 & 4 \\ -3 & 3 \end{bmatrix} \begin{bmatrix} 1 \\ 1 \end{bmatrix} = \begin{bmatrix} 8 \\ 0 \end{bmatrix} \implies u_1 = \frac{1}{4\sqrt{2}} \begin{bmatrix} 8 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \quad (\text{注意这里归一化可能有系数差异，取单位向量})$$



同理计算 $u_2$。

------

## 6. 关键性质总结

- **秩 (Rank)**: 矩阵 $A$ 的秩 $r$ 等于非零奇异值的个数。
- **子空间基 (Bases for Subspaces)**:
  - $v_1, \dots, v_r$: 行空间的标准正交基。
  - $u_1, \dots, u_r$: 列空间的标准正交基。
  - $v_{r+1}, \dots, v_n$: 零空间 ($N(A)$) 的标准正交基。
  - $u_{r+1}, \dots, u_m$: 左零空间 ($N(A^T)$) 的标准正交基。
- **数值稳定性**: SVD 将矩阵分解为有序的信息（$\sigma_1$ 包含最多信息），这使得它在图像压缩、去噪和数据分析（PCA）中极为重要。

